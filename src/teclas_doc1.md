# TECLAS.DOC: Como as BigTechs roubaram a nossa imagem

Neste primeiro episódio de TECLAS.DOC nós vamos explorar a história da coleção dos nossos dados e erosão do consentimento pelas big techs. Vamos entender como a etapa da web 2.0, armazenamento barato e "coleta de dados para melhorar os serviços" originou a era das IAs generativas.

por **bianaaf**

Leia também o artigo complementar em: <https://bianaaf.medium.com/voc%C3%AA-est%C3%A1-atrapalhando-o-desenvolvimento-tecnol%C3%B3gico-e-talvez-j%C3%A1-n%C3%A3o-possa-permanecer-como-%C3%A9-0688a02dc674>

## Introdução

Antes mesmo de você perceber, uma imagem sua já pode estar por aí. Não porque alguém decidiu expor essa imagem, mas porque em algum momento ela foi capturada, arquivada e esquecida em alguns bancos de dados. Uma foto comum ou um vídeo banal, um registro que parecia não ter importância… Porque é assim que a maioria das imagens nascem, sem uma grande intenção por trás, sem peso e, principalmente, sem a sensação de que um dia elas precisariam ser defendidas.

Muito tem se falado sobre como nossas imagens — e, inclusive, as artes de vários artistas — têm sido usadas para treinar inteligências artificiais de maneira totalmente não consensuada. Mas o que eu vou trazer neste vídeo não é exatamente a denúncia de que isso está acontecendo, porque, se você é minimamente informado, provavelmente já sabe que esse roubo de imagens existe. A intenção aqui é expor como isso está acontecendo.

Como, de repente, as big techs passaram a achar “ok” roubar milhões de imagens de milhões de pessoas, sem que essas pessoas sequer soubessem que isso estava acontecendo, e qual caminho foi traçado, tijolo por tijolo, nas últimas duas décadas, para possibilitar que hoje a Grok, do Elon Musk — só para citar um exemplo que, infelizmente, está longe de ser o único disponível — seja usada para tirar roupa de mulheres e crianças de maneira totalmente criminosa, sem que sequer exista uma forma clara de responsabilizar a empresa por isso, já que a lei teve dificuldades de acompanhar o movimento dessa construção.

Então, esse vídeo vai ser como a apresentação de um dossiê.

Não se trata aqui de uma análise estritamente jurídica, mas de uma reconstrução histórica e sociotécnica dos regimes de consentimento.

E é importante, já de cara, dizer que essa história não começa com a inteligência artificial, nem com imagens geradas por computador. Ela começa bem antes, com decisões aparentemente técnicas, termos de uso longos demais para serem lidos, caixas de consentimento marcadas quase automaticamente e uma ideia que foi se consolidando aos poucos: a de que imagens disponíveis na internet seriam, de alguma forma, imagens disponíveis para uso.

Ao longo dos anos, a imagem deixou de ser tratada como uma extensão direta da pessoa e passou a ser encarada como dado. Algo que pode ser coletado em larga escala, armazenado, classificado e reutilizado sem que isso pareça, à primeira vista, um problema jurídico ou moral. Esse deslocamento não aconteceu de uma vez. Ele foi sendo normalizado, camada por camada, em nome da inovação, da eficiência e do progresso tecnológico.

Quando percebemos, o consentimento já não era mais um requisito central, mas um detalhe formal, muitas vezes genérico o suficiente para permitir quase qualquer uso. A lei, construída para lidar com relações mais diretas e identificáveis, passou a ter dificuldade de responder a processos difusos, automatizados e massivos, nos quais não há um autor claramente visível nem uma vítima facilmente localizável.

É nesse vazio que essas tecnologias se expandem. Um espaço em que imagens circulam sem contexto, sem origem reconhecida e sem responsabilidade claramente atribuída. E é justamente esse percurso — sorrateiro, gradual e com a aparência falsa de ser “inevitável” — que este vídeo se propõe a reconstruir. Não para falar apenas do que está errado hoje, mas para entender como chegamos até aqui e por que, agora, defender a própria imagem se tornou tão mais difícil.

## Capítulo 1: O início da desconstrução do consentimento e a era dos megadatasets (2004-2007)

Para começar o nosso percurso histórico e entender a origem dessa problemática, é preciso voltar a 2004. “Nossa, Lucas, 2004? Mas o que isso tem a ver com inteligência artificial e roubo de imagens hoje?” Tem tudo a ver. Foi justamente nesse momento que as redes sociais, como passamos a conhecê-las, começaram a ganhar tração, ancoradas em uma promessa que parecia simples, razoável e favorável ao usuário: você compartilha suas imagens, suas memórias, fragmentos da sua vida — e, ainda assim, continua sendo o legítimo dono desse conteúdo. A ideia de que o usuário mantinha a propriedade sobre aquilo que publicava foi central para a construção da confiança inicial nessas plataformas.

Plataformas pioneiras como o Flickr, lançado em 2004, e o Facebook, que também iniciou suas operações nesse mesmo ano, afirmavam em seus termos de serviço e em suas comunicações públicas que não reivindicavam a propriedade formal do conteúdo postado pelos usuários. Essa ênfase na autoria individual ajudou a sustentar um discurso de respeito à propriedade intelectual e à autonomia do usuário. No entanto, uma leitura atenta dos documentos jurídicos revela uma nuance fundamental: embora a titularidade formal permanecesse com o usuário, a licença concedida às plataformas para usar esse conteúdo já era ampla desde o início.

No caso do Facebook, os Termos de Uso de 2004 previam uma licença irrevogável, perpétua e sublicenciável para utilização, reprodução e transformação do material publicado. Assim, mesmo sem reivindicar a propriedade no papel, a plataforma assegurava para si um controle extenso e duradouro sobre os conteúdos. É nesse descompasso entre o discurso de propriedade do usuário e a prática jurídica das licenças que começam a surgir, ainda de forma embrionária, os sinais da mudança de paradigma que estruturaria a economia das plataformas nos anos seguintes.

[Facebook's Original Terms of Service (Wayback Machine)](https://web.archive.org/web/20040212044345/http:/www.thefacebook.com/terms.php)

Em 2007, o Facebook deu um passo significativo e controverso ao lançar o programa Beacon. Este recurso aparentemente inovador, mas profundamente invasivo, tinha como objetivo tornar públicas as atividades dos usuários realizadas fora da plataforma — como compras em sites parceiros ou aluguel de filmes — sem que houvesse um consentimento explícito e granular para cada uma dessas divulgações. A premissa era que a atividade do usuário em sites externos seria automaticamente compartilhada com sua rede de amigos no Facebook. A reação a essa iniciativa foi massiva e esmagadoramente negativa, gerando uma onda de protestos de usuários e grupos de defesa da privacidade. A pressão pública foi tão intensa que o recurso acabou sendo descontinuado, e o próprio Mark Zuckerberg, CEO do Facebook, emitiu um pedido de desculpas público, reconhecendo o erro da empresa

[Facebook CEO Apologizes, Lets Users Turn Off Beacon (Wired)](https://www.wired.com/2007/12/facebook-ceo-apologizes-lets-users-turn-off-beacon/)

Este episódio, embora tenha resultado na retirada do Beacon, já indicava algo de extrema importância: as plataformas estavam ativamente testando os limites da privacidade e até onde poderiam ir na coleta e uso de dados sem a necessidade de pedir permissão direta e explícita. E, o mais preocupante, o episódio do Beacon revelou às plataformas os limites da reação pública, influenciando estratégias posteriores de implementação mais silenciosa.

Paralelamente aos desenvolvimentos nas redes sociais, fora das plataformas comerciais, a pesquisa acadêmica em visão computacional estava dando um salto decisivo que teria implicações profundas para o futuro da imagem digital. No final dos anos 2000, essas pesquisas entraram na era dos megadatasets “in the wild”. Essa expressão, cunhada por pesquisadores da área, designa conjuntos massivos de fotos que eram coletadas em “ambientes naturais”, ou seja, diretamente da internet aberta, sem qualquer controle ou consentimento dos sujeitos fotografados. O ano de 2007 marca um ponto de virada crucial nesse campo, quando o laboratório de visão computacional da Universidade de Massachusetts lançou o Labeled Faces in the Wild (LFW). Este foi um dataset inovador, composto por milhares de rostos coletados de imagens publicamente disponíveis na internet. Ao todo, um conjunto impressionante de 13.233 fotos de rostos, pertencentes a 5.749 pessoas distintas, foi obtido online sem o consentimento explícito dos indivíduos retratados

[Origins and Endpoints of Image Training Datasets Created “In the Wild”](https://adam.harvey.studio/origins-and-endpoints/#:~:text=News%E2%80%9D%20that%20contained%20half%20a,success%20in%20dozens%20more%20datasets)

Muitas das imagens incluídas no LFW eram de celebridades ou figuras públicas, capturadas em situações do dia a dia, em eventos ou em contextos que as tornavam publicamente acessíveis. No entanto, o aspecto mais importante e de longo alcance do LFW foi o precedente que ele estabeleceu, um precedente que se mostraria determinante para tudo o que estamos vendo e vivendo hoje no campo da inteligência artificial e da privacidade. O LFW normalizou, dentro da comunidade de pesquisa, o uso de fotografias pessoais coletadas da web sem a permissão explícita dos indivíduos. A partir de então, tornou-se uma prática “normal” e aceitável treinar algoritmos de reconhecimento facial com fotos encontradas online, tratando-as como material público disponível para qualquer finalidade de pesquisa e desenvolvimento. Essa mudança sutil, mas poderosa, na percepção do que constitui “público” e “disponível” foi um marco na despersonalização da imagem digital

[This is how we lost control of our faces (MIT Technology Review).](https://www.technologyreview.com/2021/02/05/1017388/ai-deep-learning-facial-recognition-data-history/)

## Capítulo 2: A consolidação da licença irrestrita e os megadatasets (2009)

O ano de 2009 marcou um período de intensificação dessas tendências, com o Facebook e a comunidade de pesquisa  em visão computacional dando passos que solidificariam a ideia de que o conteúdo online era, por padrão, um recurso a ser explorado.

Em 2009, o Facebook, em um movimento que muitos consideraram sorrateiro, alterou silenciosamente seus Termos de Uso. A mudança mais controversa foi a inclusão de uma cláusula que concedia à empresa uma licença perpétua, irrevogável e transferível sobre qualquer conteúdo postado pelos usuários, e o mais alarmante, essa licença permaneceria válida mesmo após a exclusão da conta do usuário. Isso significava que, uma vez que algo fosse postado no Facebook, a empresa reivindicava o direito de usá-lo para sempre, sem restrições geográficas ou temporais, e poderia até mesmo transferir esse direito a terceiros

[Facebook Follies: Why Facebook’s Recent Change to its User Agreement Was A Bad Move (FindLaw)](https://supreme.findlaw.com/legal-commentary/facebook-follies-why-facebooks-recent-change-to-its-user-agreement-was-a-bad-move-and-will-likely-be-unenforceable.html)

A reação pública a essa alteração foi imediata e extremamente forte, com usuários e grupos de defesa da privacidade expressando indignação generalizada. A pressão foi tanta que a empresa foi forçada a recuar — temporariamente, como se veria mais tarde. No entanto, a mensagem subjacente já havia sido claramente transmitida: o consentimento do usuário, que antes parecia ser um pilar inabalável, poderia ser reconfigurado e, em grande parte, anulado por meio de um contrato de termos de uso, muitas vezes lido de forma superficial ou ignorado.

Também em 2009, no campo da pesquisa em inteligência artificial, a professora Fei-Fei Li, então na Universidade de Stanford, liderou um projeto monumental que resultaria no lançamento do ImageNet. Este foi um dataset colossal, composto por mais de 14 milhões de imagens, todas coletadas da internet, com o objetivo explícito de treinar sistemas de visão computacional. O ImageNet foi meticulosamente organizado, abrangendo milhares de categorias de objetos e cenas, e sua construção se deu por meio de mecanismos de busca automatizados e da exploração de acervos públicos do Flickr. Isso significa que as imagens utilizadas eram, em grande parte, fotos que pessoas comuns haviam compartilhado online, novamente, sem qualquer consulta individual ou pedido de permissão específico para a inclusão em um dataset de treinamento de IA.

Este projeto ambicioso não apenas transformou a área de IA, mas a redefiniu: ele provou, de forma inequívoca, que a disponibilidade de dados em larga escala podia “alimentar” algoritmos com uma eficácia e precisão inéditas, impulsionando uma explosão de novas aplicações de visão computacional, que iam desde filtros fotográficos em smartphones até sistemas avançados de vigilância e reconhecimento de objetos

[ImageNet: A Large-Scale Hierarchical Image Database (IEEE)](https://ieeexplore.ieee.org/document/5206848)

Contudo, esse avanço técnico extraordinário veio acompanhado de um custo ético e social significativo: ele contribuiu para normalizar e tornar aceitável a ideia de que milhões de imagens públicas poderiam ser livremente reaproveitadas para fins de treinamento de IA, sem a necessidade de obter o consentimento dos indivíduos retratados. Essa abordagem levou a uma mudança cultural sutil, mas profundamente impactante, dentro da comunidade de pesquisa e desenvolvimento de IA. Pesquisadores passaram a ver as fotos não mais como representações de pessoas com direitos inerentes à sua imagem e privacidade, mas sim como meros dados brutos, insumos para algoritmos.

A dimensão humana por trás da imagem foi, em grande parte, desconsiderada, abrindo caminho para uma despersonalização da informação visual

[The Politics of Images in Machine Learning Training Sets  (Excavating AI)](https://www.excavating.ai/)

Em suma, esses projetos — o Labeled Faces in the Wild e o ImageNet — mudaram completamente a área de inteligência artificial, impulsionando-a para uma nova era de capacidades sem precedentes. Mas, ao mesmo tempo, eles consolidaram uma ideia perigosa e de longo alcance: a de que imagens encontradas na internet, especialmente aquelas consideradas “públicas”, seriam dados disponíveis para uso irrestrito, sem que a pessoa por trás da imagem tivesse qualquer voz ou controle sobre seu destino. A pessoa simplesmente desaparece da equação, tornando-se um mero ponto de dados em um vasto oceano de informações digitais.


## Capítulo 3: A biometria e a comercialização da imagem (2010-2013)

Até meados de 2015, empresas como Facebook e Google já haviam acumulado internamente bilhões de imagens de usuários, construindo repositórios de dados visuais de proporções gigantescas. No entanto, para compreender a evolução específica desse percurso de apropriação e uso de imagens, é necessário retroceder alguns anos em nossa cronologia e analisar os desenvolvimentos que ocorreram entre 2010 e 2013, um período crucial para a normalização do reconhecimento facial e a comercialização da imagem pessoal.

Em 2010, o Facebook deu um passo, digamos, “ousado” ao ativar, por padrão, o reconhecimento facial automático para seus usuários. Isso foi implementado através do lançamento da funcionalidade de sugestão de marcação de fotos (tag suggestions). Este sistema de reconhecimento facial analisava automaticamente todas as fotos carregadas na plataforma e sugeria a identificação dos rostos dos amigos do usuário presentes nessas imagens. O que muitos usuários não perceberam na época, ou não foram devidamente informados, é que, ao fazer isso, o Facebook estava, na prática, criando modelos biométricos faciais detalhados de milhões de pessoas. Esses “templates” biométricos eram gerados e armazenados, alimentando algoritmos de IA sem que a maioria dos usuários tivesse qualquer conhecimento ou consentimento explícito sobre essa coleta e processamento de dados tão sensíveis

[In re Facebook and the Facial Identification of Users (EPIC)](https://epic.org/documents/in-re-facebook-and-the-facial-identification-of-users/)

Essa prática não passou despercebida por grupos de privacidade. Em 2011, a Electronic Privacy Information Center (EPIC), uma renomada organização de defesa da privacidade, protocolou uma denúncia formal junto à Federal Trade Commission (FTC) dos Estados Unidos. A EPIC alegou que o Facebook estava coletando dados biométricos de forma injusta e enganosa, “sem conhecimento ou consentimento” dos usuários, ao construir um vasto banco de dados de faces identificáveis. A preocupação com essa prática não se limitou aos EUA; reguladores europeus também expressaram sérias reservas. Assim, em 2012, sob forte pressão da Irlanda, onde o Facebook mantém sua sede europeia, a empresa foi obrigada a suspender o recurso de reconhecimento facial na União Europeia

[Facebook suspends photo tag tool in Europe (BBC)](https://www.bbc.com/news/technology-19675172)

No entanto, apesar dessa restrição regional, globalmente o recurso continuou a prosperar nos anos seguintes, e em escala massiva, potencialmente envolvendo centenas de milhões de usuários, foram catalogados internamente pela plataforma, consolidando sua posição como um dos maiores repositórios de dados biométricos faciais do mundo.

Em janeiro de 2011, o Facebook introduziu outra inovação controversa: os sponsored stories (histórias patrocinadas). Este formato de anúncio exibia aos amigos de um usuário o seu nome, foto de perfil e o fato de ele ter “curtido” uma página ou feito check-in em um local, tudo isso sem qualquer consentimento explícito do usuário para ser associado a uma publicidade, e sem qualquer remuneração por esse uso de sua imagem e identidade. Na prática, o Facebook estava transformando seus usuários em garotos-propaganda involuntários para produtos e serviços, utilizando suas imagens e identidades pessoais em campanhas publicitárias. Essa “cessão compulsória de direitos de imagem” gerou uma série de ações judiciais imediatas, sendo a mais notória o caso Fraley vs. Facebook. A controvérsia legal resultou em um acordo em 2013, pelo qual o Facebook concordou em pagar US$20 milhões e, mais importante, em dar aos usuários algum controle sobre esse uso de sua imagem, permitindo-lhes optar por não aparecer em tais anúncios

[Fraley v. Facebook: Settlement in Sponsored Stories Case (Citizen.org)](https://www.citizen.org/litigation/fraley-v-facebook-6/)

Apesar do acordo, que ofereceu uma opção de opt-out, a rede já havia alterado seus documentos de política, pavimentando o caminho para futuras utilizações de dados de usuário em contextos comerciais.

Também nesse período, em 2011, o Google implementou o Find My Face como parte de seu então novo serviço de rede social, o Google+. Este recurso de reconhecimento facial tinha como objetivo “ajudar os usuários a etiquetar pessoas em fotos”. Ao ativá-lo — e é importante notar que foi oferecido como uma opção opt-in, ou seja, desligado por padrão, o que demonstrava uma abordagem mais cautelosa em relação ao consentimento do que a do Facebook na época — o usuário recebia uma notificação quando alguém enviava uma foto em que ele possivelmente aparecia, podendo então confirmar a tag com seu nome. Embora mais respeitoso em sua implementação inicial, o Find My Face representava mais um passo na normalização do reconhecimento facial em larga escala e na coleta de dados biométricos por grandes empresas de tecnologia

[Google+ adding facial recognition, deeper integration with Gmail (The Verge)](https://www.theverge.com/2011/12/9/2622091/google-plus-facial-recognition-deeper-integration-with-gmail)

—
Em 2012, após ser adquirida pelo Facebook, a plataforma de compartilhamento de fotos Instagram anunciou novos Termos de Uso que entrariam em vigor em 16 de janeiro de 2013. Entre as mudanças propostas, uma cláusula específica causou pânico generalizado entre seus usuários e na mídia. Essa cláusula dava ao Instagram o direito de transformar fotos dos usuários em anúncios, sem prévia aprovação ou qualquer compensação financeira aos criadores das imagens. Em outras palavras, a foto de um usuário poderia ser vendida, licenciada para anunciantes ou utilizada em campanhas publicitárias do próprio Instagram, sem qualquer notificação ou consentimento individual. Essa mudança foi amplamente interpretada como “o Instagram vai vender suas fotos para empresas” e representava uma cessão compulsória de direitos autorais e de imagem em benefício exclusivo da plataforma, gerando uma enorme controvérsia

[What Instagram’s New Terms of Service Mean for You (The New York Times)](https://bits.blogs.nytimes.com/2012/12/18/instagrams-new-terms-of-service-what-they-mean/)

—
A reação a essa proposta foi massiva e esmagadoramente negativa. Usuários devotos da plataforma ameaçaram excluir suas contas em massa, e celebridades e fotógrafos profissionais protestaram publicamente contra o que consideravam uma apropriação indevida de seu trabalho e imagem. Um fotógrafo chegou a chamar os novos termos de [“nota suicida”](https://www.latimes.com/entertainment/la-xpm-2012-dec-19-la-fi-instagram-backlash-20121219-story.html#:~:text=Clayton%20Cubitt%2C%2040,terms%20of%20service.) do Instagram, acusando as empresas de mídia social de adotarem uma postura predatória de “poste seu conteúdo de graça e em troca sugamos sua vida”.

Diante do clamor público — que incluiu até mesmo a meia-irmã de Mark Zuckerberg curtindo posts criticando a cláusula polêmica, o que demonstra a amplitude da insatisfação — o cofundador do Instagram, [Kevin Systrom, veio a público para tentar acalmar os ânimos](https://www.latimes.com/entertainment/la-xpm-2012-dec-19-la-fi-instagram-backlash-20121219-story.html#:~:text=Instagram%20founder%20Kevin%20Systrom%2C%20seen,photos%20are%20your%20photos.%20Period.%E2%80%9D). Ele afirmou categoricamente: “O Instagram não reivindica propriedade de suas fotos… respeitamos que suas fotos são suas. Ponto.”. Systrom atribuiu o “mal-entendido” a uma redação infeliz e garantiu que o Instagram não tinha planos de colocar fotos de usuários em anúncios sem consentimento explícito

[After Backlash, Instagram Changes Back To Original Terms Of Service (Forbes)](https://www.forbes.com/sites/tomiogeron/2012/12/20/after-backlash-instagram-changes-back-to-original-terms-of-service/)

Na prática, a empresa recuou de suas intenções originais: em 20 de dezembro de 2012, o Instagram anunciou que reverteria os termos controversos, voltando aos termos antigos de 2010. Este episódio, que ficou conhecido como “Instagate”, evidenciou o limite da tolerância pública. A tentativa explícita de suspender o consentimento e apropriar-se das imagens dos usuários para fins lucrativos encontrou forte oposição social e midiática, forçando a plataforma a reconsiderar.

Mesmo revertido, o caso serviu como um alerta claro para as demais plataformas sobre os riscos de tentar implementar mudanças draconianas nos direitos autorais e de imagem por meio de alterações nos Termos de Uso. (De fato, nos anos seguintes, a maioria das alterações semelhantes seria comunicada em linguagem mais cuidadosa ou implementada de forma discreta, para evitar revoltas e escândalos públicos.) Essa reação ensinou algo valioso às plataformas: mudanças explícitas demais, que chamam a atenção da mídia e dos usuários, geram revolta e backlash. Por outro lado, mudanças discretas, diluídas em termos de uso longos e complexos, tendem a passar quase despercebidas, permitindo a gradual erosão dos direitos dos usuários sem grande oposição

## Capítulo 4: A unificação de dados e o "interesse legítimo" (2012-2013)

O período entre 2012 e 2013 foi marcado por uma série de movimentos estratégicos por parte das grandes empresas de tecnologia, especialmente Google e Facebook, que visavam consolidar o controle sobre os dados dos usuários e expandir as bases legais para seu uso. Em 1º de março de 2012, com efetivação ao longo de 2012 e 2013, o Google implementou uma Política de Privacidade unificada que abrangia todos os seus serviços. Essa mudança permitiu à empresa combinar informações de usuários de diferentes plataformas — como Gmail, YouTube, histórico de pesquisas e Google+ — em um único perfil abrangente. Embora essa unificação fosse focada em dados pessoais em geral, ela teve um impacto significativo na capacidade do usuário de controlar isoladamente seus dados visuais em serviços específicos. Por exemplo, uma foto enviada ao Google+ poderia agora ser usada para personalizar resultados de busca no YouTube ou vice-versa, sem que o usuário tivesse controle granular sobre cada uso individual

[Google in privacy policy changes across its services](https://www.bbc.com/news/business-16713562)

Em paralelo a essa unificação, o Google atualizou em 2013 seus Termos de Serviço para explicitar e formalizar práticas de análise automatizada de conteúdo. Essa atualização veio após a empresa enfrentar ações judiciais e críticas por escanear e-mails do Gmail para fins publicitários. Para contornar essas questões legais e éticas, o Google incluiu nos termos uma cláusula abrangente que afirmava que o usuário concedia ao Google permissão para “hospedar, reproduzir, distribuir e analisar seu conteúdo” com o objetivo de operar e, crucialmente, “melhorar os serviços”. Essa linguagem — **“improve the services”** (aprimorar os serviços) — tornou-se um padrão da indústria e se mostrou de importância fundamental. Ela significava que qualquer dado do usuário, incluindo e-mails, documentos, fotos armazenadas no Google Fotos ou vídeos no YouTube, poderia ser utilizado em algoritmos de aprendizado de máquina para “aprimorar produtos” existentes ou para desenvolver novas tecnologias e funcionalidades.

Essa cláusula, portanto, abriu uma porta legal para o uso massivo de dados de usuários para o desenvolvimento de IA, mesmo que a palavra “inteligência artificial” não fosse explicitamente mencionada

[Google Updates Terms of Service to Explicitly Mention Email Scanning (The Guardian)](https://www.theguardian.com/technology/2014/apr/15/google-updates-terms-of-service-to-explicitly-mention-email-scanning/)

> Sobre esse tema da apropriação de dados para fins comerciais e de desenvolvimento tecnológico, é altamente recomendável a leitura do livro de Shoshana Zuboff, “A era do capitalismo de vigilância”. A obra explora em profundidade como as empresas de tecnologia transformaram a experiência humana em dados para extração de valor, um processo que ela denomina de “capitalismo de vigilância”.

Nesse novo ecossistema unificado do Google, o recurso Find My Face, que já havia sido implementado em 2011, tornou-se particularmente relevante. Ele exemplificava como o Google estava utilizando dados visuais de usuários (especificamente a biometria facial presente em fotos) de forma integrada ao perfil unificado do usuário. Embora o design inicial do Find My Face fosse mais respeitoso, sendo opt-in, o que evitou polêmicas imediatas, o recurso estava plenamente amparado pela nova Política de Privacidade unificada. Essa política permitiu ao Google+ cruzar dados de identidade do usuário com seu rosto em fotos para aprimorar a experiência de marcação e, implicitamente, para refinar seus algoritmos de reconhecimento facial

O Google, aproveitando-se dessa licença ampla e da unificação de dados, não demorou a dar o próximo passo. Também em 2013, a empresa atualizou seus termos para implementar os “shared endorsements” (endossos compartilhados). Este recurso consistia em anúncios que exibiam o nome e a foto do perfil dos usuários do Google e Google+ em recomendações publicitárias. Por exemplo, se um usuário avaliasse positivamente um restaurante no Google, seus contatos poderiam ver a foto e o nome dele em um anúncio daquele estabelecimento. Embora o Google tenha fornecido uma opção de opt-out para os usuários que desejassem desativar essa funcionalidade, a mudança representou outra flexibilização das garantias de controle sobre a própria imagem. Quem não percebesse a alteração ou não se desse ao trabalho de modificar as configurações de privacidade teria sua imagem e nome automaticamente atrelados a propagandas, transformando-os em embaixadores de marcas sem consentimento explícito e ativo

[Google ‘terms of service’ update allows users profile information to be used in ads](https://globalnews.ca/news/962086/google-terms-of-service-update-allows-users-profile-information-to-be-used-in-ads/#:~:text=%E2%80%9CIf%20you%20have%20a%20Google,%E2%80%9D)

[Termos de serviço do Google](https://policies.google.com/terms/archive/20131111?hl=pt-BR)

No mesmo ano de 2013, o Facebook também atualizou sua Data Use Policy (Política de Uso de Dados), esclarecendo que, ao utilizar o serviço, o usuário permitia o uso de seu nome, foto do perfil e conteúdo em conexão com anúncios ou conteúdo comercial patrocinado. Grupos de privacidade, como a EPIC, criticaram veementemente essas mudanças, apontando que elas ainda eram opt-out (ativadas por padrão) e reclamaram junto à FTC, mas sem sucesso em barrar totalmente a prática.

[Beware Facebook's New Terms of Service](https://rangefinderforum.com/threads/beware-facebooks-new-terms-of-service.139460/#:~:text=Beware%20Facebook%27s%20New%20Terms%20of,the%20service%2C%20including%20all)

O Twitter, por sua vez, adotou uma tática similar, utilizando nomes e fotos de perfil em promoted tweets (tweets promovidos) para indicar quando alguém seguido pelo usuário interagia com conteúdo patrocinado.

[Google ‘terms of service’ update allows users profile information to be used in ads](https://globalnews.ca/news/962086/google-terms-of-service-update-allows-users-profile-information-to-be-used-in-ads/#:~:text=Twitter%20also%20uses%20a%20similar%20tactic%20for%20%E2%80%9Cpromoted%20tweets.%E2%80%9D)

Assim, por volta de 2013, tanto Google quanto Facebook e outras redes sociais em ascensão ajustaram seus textos legais para autorizar, retroativamente, o uso abrangente do conteúdo do usuário em processos de machine learning (mesmo que sem citar explicitamente “IA” na época), diluindo as garantias anteriores de privacidade e controle sobre a própria imagem


## Capítulo 5: A expansão do escaneamento e seus contramovimentos (2014-2021)

Essa tendência de flexibilização dos termos de uso e apropriação de dados visuais demonstrou que, à medida que as plataformas buscavam novas formas de rentabilizar seus serviços, os direitos de imagem do usuário eram cada vez mais amplamente cedidos nos termos de serviço. Essa cessão ocorria sem a necessidade de um consentimento caso a caso, a menos que o usuário descobrisse e exercesse uma opção de opt-out muitas vezes escondida ou de difícil acesso nas configurações de privacidade.

Ao longo do período, a lógica dos megadatasets “in the wild” se consolidou. Em 2014, o lançamento do Yahoo Flickr Creative Commons 100M (YFCC100M) marcou a institucionalização da ideia de que imagens compartilhadas por usuários — desde que sob licenças Creative Commons — poderiam ser agregadas em massa para pesquisa. Embora apresentado como um recurso acadêmico, o dataset na prática, passaram a circular entre contextos acadêmicos e comerciais, dissolvendo de fato — ainda que não formalmente — essa fronteira.

A partir de 2015, com o lançamento do Google Fotos, o escaneamento automático de imagens se tornou uma funcionalidade padrão e onipresente. Este recurso foi construído sobre os avanços significativos proporcionados por datasets como o LFW e o ImageNet na década anterior, que já haviam normalizado a coleta e o processamento de imagens em larga escala. É importante destacar que, nesse período, o Google ofereceu armazenamento ilimitado de fotos, um “benefício” que se tornou um grande atrativo para milhões de usuários, mas que, na verdade, funcionava como um mecanismo para coletar ainda mais dados visuais. Esse armazenamento ilimitado, no entanto, ficou disponível somente até 2021. Todas as fotos enviadas para o Google Fotos passavam a ser analisadas por algoritmos de inteligência artificial, e, na prática, não havia uma opção funcional de usar o serviço sem que as imagens fossem processadas por algoritmos de visão computacional

[Google Photos and the End of Unlimited Storage (The Verge)](https://www.theverge.com/2020/11/11/21560810/google-photos-unlimited-free-storage-ending-june-2021/)

Na prática, aceitar os termos do Google Fotos significava concordar tacitamente que o serviço processaria todas as imagens enviadas. Isso era apresentado como uma necessidade para o funcionamento de recursos como busca inteligente, organização automática e agrupamento de rostos. Fontes da época notaram que os usuários não recebiam um consentimento explícito e separado para essa varredura abrangente de conteúdo, nem havia uma ferramenta específica para excluir apenas os “faceprints” (modelos biométricos faciais) ou os metadados gerados pela IA.

Essa lacuna no consentimento e no controle do usuário ficou evidente quando uma ação coletiva nos EUA alegou violação de privacidade biométrica contra o Google. A empresa acabou concordando em informar melhor os usuários e permitir o controle sobre o agrupamento de rostos, mas o processamento básico das imagens para fins de IA continuou sendo o motor fundamental do serviço, demonstrando a dificuldade de reverter a lógica de apropriação de dados uma vez estabelecida

[Google to pay $100 million to Illinois residents for Photos’ face grouping feature](https://www.theverge.com/2022/6/6/23156198/google-class-action-face-grouping-biometric-information-illinois-privacy-act)

Essa dinâmica se intensificou no campo do reconhecimento facial.
Em 2016, surgiram datasets emblemáticos como o MegaFace e o MS-Celeb-1M, ambos construídos a partir de imagens coletadas na internet, sobretudo do Flickr e de buscas automatizadas. Milhões de rostos foram indexados sem consulta às pessoas retratadas. No caso do MegaFace, exigências das licenças — como atribuição e restrição ao uso comercial — foram ignoradas, e o dataset tornou-se base para benchmarks usados por grandes empresas de tecnologia. Já o MS-Celeb-1M, criado pela Microsoft, incluiu não apenas celebridades, mas jornalistas, acadêmicos e indivíduos comuns, sendo utilizado inclusive em pesquisas com aplicações militares e de vigilância. Pouquíssimas pessoas sabiam que seus rostos estavam nessas bases; nenhuma foi consultada.

[MegaFace](https://exposing.ai/megaface/#:~:text=But%20the%20assumptions%20around%20sharing,in%20achieving%20research)

[Facial Recognition Database Facing Potential Legal Action For Using Photos, Many of Children, Without Permission](https://securitytoday.com/articles/2019/10/14/facial-recognition-database-facing-potential-legal-action-for-using-photos.aspx#:~:text=A%20facial%20recognition%20database%20holding,the%20express%20permission%20of%20users)

[Microsoft Scraps 10 Million Facial Recognition Photos On The Low](https://www.forbes.com/sites/korihale/2019/06/25/microsoft-scraps-10-million-facial-recognition-photos-on-the-low/)

[MS-Celeb-1M (MS1M)](https://exposing.ai/msceleb/#:~:text=After%20the%20MS%20Celeb%20dataset,applications%20in%203D%20face%20recognition)

A partir de meados da década, escândalos sucessivos começaram a mudar o clima regulatório e a percepção pública. Em 2017, o Facebook reintroduziu globalmente o reconhecimento facial automático com o recurso Photo Review, revelando que já havia criado templates faciais em larga escala a partir das fotos de seus usuários. Embora apresentado como uma ferramenta de proteção, o sistema operava por padrão e se apoiava em justificativas amplas nos termos de uso, o que levou a disputas jurídicas — como a ação baseada na lei biométrica de Illinois (BIPA), encerrada por meio de um acordo de centenas de milhões de dólares, negociado em 2020 e aprovado judicialmente em 2021.

Em 2018, dois marcos aprofundaram essa inflexão. De um lado, entrou em vigor o GDPR na União Europeia, impondo regras mais rígidas de consentimento e transparência. De outro, o escândalo Cambridge Analytica escancarou como dados do Facebook — incluindo fotos e perfis — haviam sido explorados politicamente em larga escala. As plataformas reagiram com mudanças nos termos, mais transparência formal e alguns controles ao usuário, mas mantiveram a lógica central: licenças amplas para uso de dados sob justificativas genéricas como “melhorar serviços” e “desenvolver IA”.

Um detalhe simbólico desse período foi a alteração do código de conduta do Google em 2018, removendo a famosa frase “Don’t be evil”. Embora não seja termo de uso, muitos viram isso como reflexo de uma mudança cultural nas big techs – prioridade à coleta e uso de dados massivos (inclusive imagens) mesmo que gere desconforto ético, desde que dentro da lei. Assim, é possível concluir que, com os holofotes da Cambridge Analytica (2018) e outros escândalos, as empresas reforçaram compliance, mas não recuaram de suas políticas de obter o máximo de direitos sobre dados do usuário.

Nesse contexto, práticas antes naturalizadas passaram a sofrer mudanças. Em 2019, a Microsoft retirou do ar o MS-Celeb-1M, reconhecendo que sua coleta não se alinhava mais a padrões éticos e legais emergentes. Ainda assim, não houve qualquer mecanismo técnico conhecido para reverter modelos já treinados, ilustrando um padrão recorrente da década: usar dados massivamente primeiro, lidar com as consequências depois.

Além do já citado fim do MS Celeb em 2019, esse ano viu aumentarem as denúncias sobre uso indevido de fotos de plataformas em projetos de IA. Em janeiro de 2019, a IBM anunciou o Diversity in Faces, um dataset de treinamento visando melhorar a equidade do reconhecimento facial. A IBM usou aprox. 1 milhão de fotos do Flickr (extraídas do YFCC100M) para compor o conjunto. Embora as imagens tivessem licença aberta, a notícia provocou manchetes como [“IBM utilizou fotos do Flickr sem consentimento para projeto de reconhecimento facial”](https://olhardigital.com.br/2019/03/13/noticias/ibm-usou-foto-das-pessoas-no-flickr-sem-autorizacao-para-treinamentos-de-faceid/). Diversos fotógrafos ficaram chocados ao descobrir seus retratos em um pacote da IBM vendido como “diverso” – muitos nunca imaginaram que um upload no Flickr (mesmo sob Creative Commons) acabaria treinando algoritmos corporativos. A IBM se defendeu alegando seguir as licenças e “melhores práticas” e ofereceu um processo de opt-out complicado (envolvendo e-mail) para autores que quisessem remover suas fotos. Aqui, vemos nitidamente a questão da “suspensão histórica do consentimento”: um usuário em 2009 marcava sua foto como CC-BY no Flickr para permitir compartilhamento pessoal, e uma década depois a foto estava sendo usada para refinar sistemas de vigilância facial – usos inimagináveis na origem. Isso expôs a lacuna entre consentimento nominal/legal e consentimento real/percebido.

Em agosto de 2020, um tribunal britânico considerou ilegal o uso de reconhecimento facial em tempo real pela South Wales Police, ao concluir que a tecnologia violava direitos fundamentais à privacidade e à proteção de dados, além de carecer de salvaguardas proporcionais e critérios claros de limitação. A decisão reconheceu que a coleta biométrica indiscriminada de rostos em espaços públicos configurava uma forma de vigilância excessiva e incompatível com os princípios do direito europeu, estabelecendo um precedente relevante para a contenção do uso estatal dessas tecnologias no continente. O caso evidenciou que, mesmo em contextos democráticos, o reconhecimento facial pode ultrapassar rapidamente os limites legais quando implementado em larga escala e sem controle rigoroso.

No contexto dos protestos antirracistas de 2020, a Amazon anunciou uma moratória de um ano no uso policial de seu sistema de reconhecimento facial Rekognition, citando a necessidade de um debate mais amplo e de regulamentação específica. Em maio de 2021, a empresa estendeu essa moratória por tempo indeterminado, condicionando qualquer retomada à existência de uma lei federal clara nos Estados Unidos. A decisão não decorreu de uma condenação judicial direta, mas de intensa pressão pública, acadêmica e de organizações de direitos civis, que vinham denunciando vieses raciais e riscos de abuso associados ao uso da biometria por forças de segurança. O episódio ilustra como a contestação social passou a operar como um freio relevante — ainda que instável — à adoção irrestrita dessas tecnologias.

[Amazon extends moratorium on police use of facial recognition software](https://www.reuters.com/technology/exclusive-amazon-extends-moratorium-police-use-facial-recognition-software-2021-05-18/#:~:text=May%2018%20%28Reuters%29%20,of%20its%20facial%20recognition%20software)

Porém, um evento que acontecia simultâneamente chama atenção, indo no movimento contrário dessa “mudança de clima regulatório”. Inicia-se, também ali, a era dos deepfakes. Em 2017, um usuário do Reddit apelidado “Deepfakes” divulga vídeos pornográficos falsos em que rostos de celebridades foram inseridos digitalmente. Essas montagens realistas, viabilizadas por algoritmos de aprendizado profundo e GANs, chocam o público: pela primeira vez, pessoas veem seus rostos usados sem qualquer autorização para gerar conteúdos enganosos e potencialmente destrutivos.

[A Brief History of Deepfakes](http://realitydefender.com/insights/history-of-deepfakes)

Em 2018, o termo “deepfake” espalha-se, e plataformas correm para proibir pornografia involuntária criada por IA. O fenômeno evidencia o lado mais sombrio da IA generativa: fotos pessoais disponíveis online (de atores, influenciadores ou pessoas comuns) viram matéria-prima para falsificações visuais, sem que os envolvidos tenham conhecimento ou consentimento. A reação social e legislativa engatinha – nesse ano, especialistas soam alarmes e alguns estados nos EUA começam a propor leis contra deepfakes maliciosos. Nasce assim uma nova frente de debate sobre consentimento: não apenas no treinamento de IA, mas também em seu produto final.

Nos anos 2020, esse processo entra em uma nova fase com a IA generativa, quando plataformas passam a declarar explicitamente que conteúdos públicos podem ser usados para treinar modelos. É nesse cenário que casos como o da Clearview AI tornam explícita, de forma radical, a lógica da suspensão coletiva do consentimento que vinha sendo construída desde a década anterior.

O ano de 2020 trouxe um choque de realidade e expôs a fragilidade do sistema de privacidade com o escândalo do caso Clearview AI. Uma investigação aprofundada do New York Times revelou que essa empresa havia raspado mais de 3 bilhões de fotos de redes sociais abertas, como Facebook, Instagram, YouTube e LinkedIn. O objetivo era criar um vasto motor de busca facial, que era então vendido para forças policiais e agências governamentais. O escândalo não apenas revelou a escala da coleta de dados, mas também expôs uma hipocrisia latente: embora as próprias plataformas proibissem a raspagem de dados em seus termos de serviço, elas mesmas haviam tornado as fotos de perfil e outras imagens públicas por padrão durante anos, facilitando indiretamente a ação da Clearview. A Clearview AI, por sua vez, argumentou que estava apenas coletando “informações públicas”, um argumento que ecoava a lógica e os precedentes estabelecidos pelos datasets de pesquisa de 2007 (LFW) e 2009 (ImageNet), reforçando a ideia de que o que está online é de domínio público e, portanto, passível de uso irrestrito

[Clearview AI ordered to comply with recommendations to stop collecting, sharing images.](http://priv.gc.ca/en/opc-news/news-and-announcements/2021/an_211214/#:~:text=U.S.,databank%20using%20facial%20recognition%20technology)

Vale notar o contexto mais amplo desse episódio: as fotos de perfil e os posts dos usuários só estavam tão facilmente acessíveis a ferramentas como a Clearview AI porque as próprias plataformas as haviam tornado públicas por padrão ao longo dos anos. Isso remonta, por exemplo, à mudança nos termos de uso do Facebook em 2009, que permitiu que até mesmo conteúdo deletado permanecesse sob licença da empresa. Em outras palavras, a proteção à privacidade que as plataformas invocavam contra a Clearview era, em certa medida, relativa e seletiva. As empresas queriam impedir que terceiros se aproveitassem dos dados que elas mesmas haviam tornado públicos, mas mantinham para si o direito exclusivo de explorar esses mesmos dados. O caso Clearview gerou consequências significativas. Autoridades locais e nacionais em diversos países começaram a agir, e grandes empresas de tecnologia como Google, YouTube, Venmo e LinkedIn enviaram cartas de “cease and desist” (cessar e desistir) à Clearview AI, exigindo que parassem de raspar seus dados. No entanto, o dano já estava feito, e o precedente de que “o que está na web é dado público” foi novamente testado e, em muitos aspectos, reforçado nos tribunais e na percepção pública

[Google, YouTube, Venmo and LinkedIn send cease-and-desist letters to Clearview AI (CBS News)](https://www.cbsnews.com/news/clearview-ai-google-youtube-send-cease-and-desist-letter-to-facial-recognition-app/)

## Capítulo 6: A era da IA generativa (2022-2025)

O lançamento público do ChatGPT, pela OpenAI, em novembro de 2022, marcou um ponto de inflexão na percepção social da inteligência artificial, ao tornar modelos avançados de linguagem acessíveis a milhões de pessoas em poucos dias. Em paralelo, sistemas de geração de imagens como DALL·E e Stable Diffusion popularizaram o uso de modelos treinados em enormes volumes de dados visuais extraídos da internet. Essa súbita ampliação de escala e visibilidade deslocou o debate sobre IA do campo técnico para o centro da esfera pública, trazendo à tona questões até então restritas a círculos especializados: a origem dos dados de treinamento, a ausência de consentimento dos autores e retratados, e a opacidade dos processos algorítmicos. A IA generativa, assim, não inaugurou o problema do uso massivo de dados, mas tornou suas consequências socialmente incontornáveis.

[Explainer: ChatGPT - what is OpenAI’s chatbot and what is it used for?](https://www.reuters.com/technology/chatgpt-what-is-openais-chatbot-what-is-it-used-2022-12-05/#:~:text=Dec%205%20%28Reuters%29%20,conversation%20based%20on%20user%20prompts)

Também em 2022, o lançamento do dataset LAION-5B marcou um ponto de inflexão no debate sobre consentimento e captura de imagens na era da inteligência artificial generativa. Com 5,8 bilhões de pares imagem-texto extraídos da web aberta, o conjunto — organizado pela associação alemã LAION (Large-scale AI Open Network) — foi montado a partir de raspagens automatizadas do Common Crawl, abrangendo desde portais de notícia até redes sociais, blogs pessoais e bancos de imagens. Sem exigir autenticação, o sistema coletou indiscriminadamente qualquer conteúdo visível online, aplicando filtros mínimos (como exclusão de pornografia e duplicatas), mas sem excluir imagens de pessoas comuns, situações sensíveis ou contextos médicos. A falácia do “público = consentido” mostrou sua face mais crua nesse processo: uma artista californiana, por exemplo, descobriu que fotos íntimas de seu prontuário médico — tiradas por um cirurgião já falecido — haviam parado no dataset e, subsequentemente, em modelos como o Stable Diffusion. Ao buscar a remoção, recebeu como resposta que o LAION não hospeda imagens, apenas referencia URLs, remetendo-a a sites obscuros e sem interlocutores. Esse caso extremo ilustra o paradigma contemporâneo da coleta massiva: quando tudo que é tecnicamente acessível passa a ser considerado utilizável, o consentimento deixa de ser um critério ético para tornar-se um obstáculo técnico secundário. LAION-5B é mais do que um repositório — é o espelho de uma estrutura que privilegia escala, legalismo técnico e exploração “pública” sobre autonomia individual, escancarando como imagens íntimas e dados sensíveis foram subsumidos silenciosamente às engrenagens da IA generativa.

[LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS](https://laion.ai/blog/laion-5b/)

Enfim, chegamos a 2023, um ano que marcou a explosão e a popularização massiva da inteligência artificial generativa. A corrida tecnológica para treinar modelos de IA cada vez mais sofisticados, como o GPT-4, Midjourney e Grok, exigiu uma quantidade de dados sem precedentes, impulsionando as empresas a buscar e utilizar vastos repositórios de informações digitais.

Nesse contexto, em 2023, a X Corp (antigo Twitter), sob a liderança de Elon Musk, atualizou sua política de privacidade para permitir explicitamente o uso de posts públicos dos usuários para treinar seus modelos de IA. Além disso, a X Corp confirmou que coleta dados biométricos (como a foto do rosto no perfil para verificação de identidade) e que pode compartilhar esses dados com parceiros, consolidando ainda mais a prática de usar informações pessoais para o desenvolvimento de IA

[X’s privacy policy confirms it will use public data to train AI models.](https://techcrunch.com/2023/09/01/xs-privacy-policy-confirms-it-will-use-public-data-to-train-ai-models/)

Nesse mesmo período de intensa inovação e redefinição de políticas, a Meta e o LinkedIn também ajustaram suas políticas de privacidade e termos de serviço. A Meta comunicou, em 2023, seus planos de permitir que os usuários escolhessem se seus posts públicos poderiam ou não ser usados em modelos de IA. No entanto, essa opção de escolha foi limitada a residentes de algumas jurisdições, como a União Europeia, onde leis de proteção de dados mais rígidas exigiam tal concessão. Notavelmente, documentos vazados revelaram que a Meta argumenta querer usar o conceito de “interesse legítimo” como base legal para o processamento de dados, em vez de buscar o consentimento explícito dos usuários. A justificativa interna para essa abordagem é que “simplesmente considerar todos dentro e dar opção de sair” (ou seja, um modelo opt-out) é muito mais viável e menos oneroso para seus negócios do que um modelo opt-in, que exigiria o consentimento ativo de cada usuário

[Meta Usará Postagens Públicas e Interações de IA para Treinar Modelos na UE.](https://forbes.com.br/forbes-tech/2025/04/meta-usara-postagens-publicas-e-interacoes-de-ia-para-treinar-modelos-na-ue/)

No caso do LinkedIn, a empresa estabeleceu um prazo até 2024 para que os usuários solicitassem a exclusão de seus dados de certos conjuntos de treinamento de IA. Contudo, o LinkedIn enfatizou que considerava esse processamento de dados como sendo de “interesse legítimo” da empresa e, portanto, não pediria permissão explícita aos usuários. Aqueles que não concordassem com essa prática teriam que preencher um formulário de opt-out para solicitar a exclusão. Até 2025, esse mecanismo de opt-out vinha sendo implementado de forma complexa e pouco divulgada, a ponto de orientações oficiais alertarem que quem não se opusesse até uma determinada data “terá seus dados passados incluídos no treinamento e isso não poderá ser revertido depois”. Em suma, tanto LinkedIn quanto Meta sincronizaram suas políticas com a nova era da IA generativa, passando a permitir explicitamente o uso abrangente dos dados dos usuários no treinamento de IA, concedendo, no máximo, escapes burocráticos e limitados para cumprir requisitos legais em algumas regiões específicas

[LinkedIn\'s New Terms — You can opt out of AI training (LinkedIn Pulse)](https://www.linkedin.com/pulse/linkedins-new-terms-you-can-opt-out-ai-training-2025-david-petherick-vta6e/)

A partir de 2023, o uso de imagens protegidas por direitos autorais no treinamento de modelos de IA passou a ser objeto de disputas judiciais diretas. Um dos casos mais emblemáticos foi a ação movida pela Getty Images contra a Stability AI, na qual a Getty alegou o uso não autorizado de milhões de imagens de seu acervo para treinar modelos de geração visual. Embora decisões judiciais posteriores, incluindo julgamentos no Reino Unido em 2025, tenham produzido resultados parciais e ambíguos, o caso evidenciou o choque entre regimes tradicionais de propriedade intelectual e a lógica expansiva da IA contemporânea. A judicialização expôs, sobretudo, a ausência de consensos legais claros sobre o que constitui uso legítimo de dados em processos de aprendizado de máquina.

[AI firm wins high court ruling after photo agency’s copyright claim](https://www.theguardian.com/media/2025/nov/04/stabilty-ai-high-court-getty-images-copyright#:~:text=Getty%20Images%20sued%20Stability%20AI,copied%20millions%20of%20its%20images)

Ao aproximar-se de 2025, o panorama consolidado é o seguinte: praticamente todo o conteúdo público (e uma parte significativa do conteúdo privado) que os usuários geram nas grandes plataformas digitais pode ser tratado como insumo valioso para o treinamento de inteligências artificiais.

Em paralelo às disputas judiciais e aos conflitos regulatórios, organismos multilaterais e fóruns técnicos passaram a articular respostas normativas não vinculantes à expansão da IA. Iniciativas lideradas por instituições como a UNESCO e a OCDE propuseram princípios éticos para o desenvolvimento e o treinamento de sistemas de IA, enfatizando práticas como minimização de dados, anonimização, transparência e consentimento informado. Embora essas recomendações não tenham força legal direta, elas sinalizam um esforço crescente de reintroduzir limites coletivos em um ecossistema tecnológico historicamente orientado pela maximização da coleta e pelo legalismo contratual. Esses consórcios funcionam, assim, como espaços de disputa simbólica e política sobre o futuro da governança dos dados.

Essa realidade só encontra restrições quando leis locais específicas impõem limites claros e rigorosos. A década de confrontos, ajustes e redefinições de políticas que narramos estabeleceu um novo e desequilibrado poder sobre os dados.

Os Termos de Uso, que no início dos anos 2000 prometiam “suas fotos são suas, e nós só exibimos”, evoluíram para “suas fotos são suas, mas nos dê uma licença mundial, irrevogável e gratuita para qualquer uso” nos anos 2020. E agora, com a intensa corrida pelo desenvolvimento da IA, essa licença inclui expressamente o direito de treinar modelos de IA proprietários, solidificando a apropriação de dados como uma prática padrão da indústria

Conclusão: O fim do consentimento?
No campo jurídico e regulatório, observamos abordagens notavelmente divergentes em diferentes partes do mundo. A União Europeia, por exemplo, tem aperfeiçoado continuamente seu arsenal regulatório para proteger a privacidade e os dados dos cidadãos. Em 2024, implementou a Lei de Serviços Digitais (DSA) e a Lei de Mercados Digitais (DMA), e está avançando para aprovar, em 2025, a Lei de IA (AI Act), que trará regras específicas e rigorosas para o uso de dados sensíveis em algoritmos de inteligência artificial.

A UE também tem exercido pressão significativa sobre as grandes empresas de tecnologia por meio de acordos e exigências. Por exemplo, em 2024, Irlanda e Malta exigiram explicações detalhadas da Meta sobre o uso de dados para IA generativa, o que resultou naquele opt-out parcial mencionado anteriormente.

Nos Estados Unidos, por outro lado, a situação é bem diferente: ainda não existe uma lei federal abrangente de privacidade de dados. A discussão sobre o tema continua no Congresso, mas sem um consenso claro, com muitas propostas esbarrando no forte lobby da indústria de tecnologia e em divergências partidárias sobre o quão rígidas as regras deveriam ser. O governo Biden lançou em 2022 uma iniciativa chamada “Blueprint for an AI Bill of Rights”, que recomenda princípios para uma IA confiável e ética, e em 2023 assinou uma ordem executiva sobre IA segura. No entanto, nenhuma dessas iniciativas impôs limites concretos e vinculativos ao uso de dados privados em IA. Assim, coube a estados individuais e a cortes decidirem casos específicos, como a Suprema Corte possivelmente decidindo sobre as disputas de direito autoral em datasets de IA nos próximos anos, demonstrando uma abordagem fragmentada e reativa à regulação

Do ponto de vista geopolítico, o papel da China tornou-se um ponto recorrente de referência nos debates globais sobre privacidade, dados e inteligência artificial. Ao longo da última década — e com especial intensidade a partir de 2024 — parte significativa da mídia e do discurso político ocidental passou a enfatizar os sistemas chineses de integração entre bases estatais de dados, tecnologias de vigilância e programas de governança algorítmica, como o crédito social. Essas leituras tendem a apresentar a China como um exemplo de como a centralização de grandes volumes de dados pode ser convertida em capacidade técnica e vantagem estratégica no desenvolvimento de IA. No entanto, mais do que uma descrição neutra da realidade chinesa, essa comparação tem sido frequentemente mobilizada como argumento retórico por governos e empresas ocidentais para relativizar exigências internas de consentimento, transparência e limitação do uso de dados. A ideia de que a liderança futura na economia, na defesa e na inovação dependerá do acesso irrestrito a grandes massas de dados reforçou a centralidade da narrativa de “segurança nacional” no debate público. Nesse contexto, medidas como o controle ou a restrição de tecnologias estrangeiras — a exemplo das controvérsias em torno do TikTok — convivem com a flexibilização das salvaguardas aplicadas às empresas nacionais, frequentemente à custa do consentimento individual e da proteção da privacidade dos cidadãos.

E, diante de todo esse cenário complexo e multifacetado, a responsabilização por usos indevidos da imagem e dos dados pessoais permanece difusa e difícil de ser atribuída. As empresas de tecnologia frequentemente alegam que apenas forneceram as ferramentas, e que a responsabilidade recai sobre os usuários ou sobre terceiros que utilizam suas plataformas. Os termos de uso, cada vez mais longos e complexos, são projetados para diluir a culpa e transferir a responsabilidade para o usuário. A lei, por sua vez, invariavelmente chega depois, tentando regular um cenário que já se transformou e se consolidou.

O que este vídeo propõe, portanto, é olhar para esse percurso com cuidado e atenção. É fundamental entender que nada disso aconteceu de repente, como um evento isolado. Pelo contrário, a suspensão gradual do consentimento foi um processo construído passo a passo, normalizado por práticas da indústria, contratualizado por termos de uso e automatizado por tecnologias de IA.

E talvez a pergunta mais incômoda e pertinente que devemos fazer não seja simplesmente “quem roubou sua imagem?”. Mas sim, “quando foi decidido que pedir permissão deixou de ser necessário?” __(Shoshana Zuboff)__.

## Referências para todas imagens usadas:

<https://youtube.com/watch?v=RJlcdfFuqok> - Amazon Rekognition Video の紹介 ｜ AWS
<https://youtube.com/watch?v=BWQyI9cfNzY> - BCLM Fitting in the LFW
<https://youtube.com/watch?v=DUwDqSY8StE> - Can artists protect their work from AI? – BBC News
<https://youtube.com/watch?v=1leG68gw_zo> - CEO of TikTok attends Trump inauguration, but Shou Chew doesn't sit with Elon Musk & Mark Zuckerberg
<https://youtube.com/watch?v=-JkBM8n8ixI> - CEO speaks out about Clearview AI's controversial facial recognition technology
<https://youtube.com/watch?v=TGtZ5FwLb_I> - Clicking "like" on Facebook can lead to having personal info collected
<https://youtube.com/watch?v=xhxZ08E2xGI> - Facebook CTO: We didn't read the terms and conditions
<https://youtube.com/watch?v=mtHnnhbcSNw> - Facebook To Shut Down Facial Recognition System, Delete 1 Billion Face Images
<https://youtube.com/watch?v=nzy8IcwqQ0Y> - Facebook's New Sponsored Stories
<https://youtube.com/watch?v=FsoZWyBFtM4> - Google lawsuit settlement: Judge approves payout for Illinois residentes
<https://youtube.com/watch?v=KGghlPmebCY> - Google Privacy Policy Update
<https://youtube.com/watch?v=9UeOwvoOsDw> - How Google solved our photo backup nightmare
<https://youtube.com/watch?v=Pc2aJxnmzh0> - How Snapchat's filters work
<https://youtube.com/watch?v=196Dl3VSwtA> - How to opt out of Google shared endorsements
<https://youtube.com/watch?v=Vj17JK3J1JU> - ImageNet Challenge 2015 Object Detection from Video
<https://youtube.com/watch?v=HlIuclRQpxk> - Instagram policy outrage
<https://youtube.com/watch?v=S8TJer9iiJA> - Instagram Walks Back Proposed Changes
<https://youtube.com/watch?v=gLoI9hAX9dw> - It’s Getting Harder to Spot a Deep Fake Video
<https://youtube.com/watch?v=9T45bs9di_U> - Learn More About Facebook AI Research
<https://youtube.com/watch?v=NH4VW8vdD5Y> - LinkedIn is using your data to train AI. This is how to opt out
<https://youtube.com/watch?v=KLGqy7aS1mA> - Meta hit with record £1bn fine for breach of EU data regulations
<https://youtube.com/watch?v=VQFhgk_rn6o> - New Facebook Feature: Cool or Creepy?
<https://youtube.com/watch?v=xoTHXsmXWgw> - Police don’t want you to know about this facial recognition trial
<https://youtube.com/watch?v=5aOkIauvsow> - The Race To Regulate AI
<https://youtube.com/watch?v=5NpQL4q77a8> - Visual Layer Hierarchical Embedding of LAION dataset
<https://youtube.com/watch?v=41mGD1GW_ck> - What exactly happens when you click "I Agree" on Facebook?
<https://youtube.com/watch?v=anLXGXfgeUE> - X privacy policy changing, exposing more personal user information
<https://youtube.com/watch?v=SRctRjyyF6g> - X's new privacy policy allows it to collect users' biometric data
<https://youtube.com/watch?v=0jEKJsVxdDo> - Zuckerberg: Beacon Needs Work (CBS News)
<https://nytimes.com/video/opinion/100000006687787/facial-recognition-police.html> - You’re in a Police Lineup, Right Now
